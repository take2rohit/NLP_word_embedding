{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.cuda.get_device_name(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('vocabulary.pickle', 'rb') as handle:\n",
    "    vocab = pickle.load(handle)\n",
    "\n",
    "with open('bigrams_dataset.pickle', 'rb') as handle:\n",
    "    bigrams_dataset = pickle.load(handle)\n",
    "\n",
    "word_to_ix = {word: i for i, word in enumerate(vocab)}\n",
    "ix_to_word = { i: word for i, word in enumerate(vocab)}\n",
    "\n",
    "x_data = []\n",
    "y_data = []\n",
    "for bigram in bigrams_dataset:\n",
    "    x_data.append(word_to_ix[bigram[0]])\n",
    "    y_data.append(word_to_ix[bigram[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Word_dataset(Dataset):\n",
    "    \n",
    "    def __init__(self, x_data, y_data):\n",
    "        self.x_data = torch.tensor(x_data).to(device)\n",
    "        self.y_data = torch.tensor(y_data).to(device)\n",
    "        self.length = len(x_data)\n",
    "        \n",
    "    def __getitem__(self,index):\n",
    "        return self.x_data[index], self.y_data[index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "    \n",
    "train_dataset = Word_dataset(x_data, y_data)\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size = 2**10, shuffle=True)\n",
    "\n",
    "iterator = iter(train_loader)\n",
    "x, y = iterator.next()\n",
    "(x.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipGramLanguageModeler(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super().__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.linear1 = nn.Linear(embedding_dim, vocab_size)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        embeds = self.embeddings(inputs)\n",
    "        out = self.linear1(embeds)\n",
    "        log_probs = F.log_softmax(out, dim=1)\n",
    "        return log_probs\n",
    "\n",
    "EMBEDDING_DIM = 300\n",
    "net = SkipGramLanguageModeler(len(vocab), EMBEDDING_DIM)\n",
    "net.to(device)\n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = torch.optim.Adam(net.parameters())\n",
    "net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = []\n",
    "\n",
    "iteration = 10000\n",
    "\n",
    "for epoch in range(iteration):\n",
    "    total_loss = 0\n",
    "    \n",
    "    for data in train_loader:\n",
    "        x, y = data\n",
    "        x, y = Variable(x).to(device), Variable(y).to(device)\n",
    "        log_probs = net.forward(x)\n",
    "        loss = criterion(log_probs, y).to(device)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        net.zero_grad()\n",
    "        \n",
    "    losses.append(total_loss)\n",
    "    \n",
    "    if epoch %10 == 0:\n",
    "        print('Epoch: {} Loss: {}'.format(epoch, total_loss))\n",
    "    \n",
    "    if epoch % 100 == 0:\n",
    "        PATH = './saved_model.pth'\n",
    "        torch.save(net.state_dict(), PATH)\n",
    "        print(\"Model Saved! at epoch no.: {}\".format(epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(PATH))\n",
    "initial = 'if'\n",
    "initial2 = 'thou'\n",
    "print(initial,initial2, end=\" \")\n",
    "for i in range(22):\n",
    "    outputs = model(torch.tensor([word_to_ix[initial],word_to_ix[initial2]]).cuda())\n",
    "    _, predicted = torch.max(outputs, 1)\n",
    "    word_pred = ix_to_word[int(predicted.cpu().numpy())]\n",
    "    initial = initial2\n",
    "    initial2 = word_pred\n",
    "    print(word_pred, end=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "op = net.forward(x[0])\n",
    "criterion(op, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
